<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>决策树分类代码实现 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="决策树决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。 1 属性选择度量属性选择度量是一种选择分类准则，把给定类标记的训练元祖的数据分区D“最好地”划分成单独类的启发式方法。  信息增益  ID3使用信息增益作为属性选择度量 增益率 ID3后继C4.">
<meta name="keywords" content="分类,决策树">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树分类代码实现">
<meta property="og:url" content="http://yoursite.com/2018/08/09/MachineLearningAndDataMining/决策树/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="决策树决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。 1 属性选择度量属性选择度量是一种选择分类准则，把给定类标记的训练元祖的数据分区D“最好地”划分成单独类的启发式方法。  信息增益  ID3使用信息增益作为属性选择度量 增益率 ID3后继C4.">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-08-15T09:11:59.882Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树分类代码实现">
<meta name="twitter:description" content="决策树决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。 1 属性选择度量属性选择度量是一种选择分类准则，把给定类标记的训练元祖的数据分区D“最好地”划分成单独类的启发式方法。  信息增益  ID3使用信息增益作为属性选择度量 增益率 ID3后继C4.">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-MachineLearningAndDataMining/决策树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/09/MachineLearningAndDataMining/决策树/" class="article-date">
  <time datetime="2018-08-09T10:41:51.000Z" itemprop="datePublished">2018-08-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/数据挖掘/">数据挖掘</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      决策树分类代码实现
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。</p>
<h1 id="1-属性选择度量"><a href="#1-属性选择度量" class="headerlink" title="1 属性选择度量"></a>1 属性选择度量</h1><p>属性选择度量是一种选择分类准则，把给定类标记的训练元祖的数据分区D“最好地”划分成单独类的启发式方法。</p>
<ul>
<li>信息增益  ID3使用信息增益作为属性选择度量</li>
<li>增益率 ID3后继C4.5使用它作为信息增益的扩充，试图克服一种偏倚</li>
<li>基尼指数 在CART中使用，基尼指数考虑每个属性的二元划分</li>
</ul>
<p>本文将使用信息增益作为属性的选择度量，下面是计算序列的信息熵的代码，参数attr_list代表的是该属性列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *;</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict,namedtuple</span><br><span class="line">filename = <span class="string">r'C:\Users\14259\Desktop\24周\electronics.csv'</span>;</span><br><span class="line">feat_names = [<span class="string">'RID'</span>,<span class="string">'age'</span>,<span class="string">'income'</span>,<span class="string">'student'</span>,<span class="string">'credit_rating'</span>];</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    csv_reader = csv.reader(open(filename,<span class="string">'r'</span>));</span><br><span class="line">    dataset = list(csv_reader)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集中属性和labels切分开</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_lables</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataattrs = list(zip(*dataset))</span><br><span class="line">    labels = dataattrs[<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">del</span> item[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset,labels</span><br><span class="line"></span><br><span class="line">dataset = load_data(filename);</span><br><span class="line">print(<span class="string">"data size is %d lines"</span> % len(dataset))</span><br><span class="line">dataset,labels = split_lables(dataset)</span><br><span class="line"><span class="comment"># print(dataset,labels)</span></span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># 计算信息熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_shannon_entropy</span><span class="params">(attr_list)</span>:</span></span><br><span class="line">    attrs = set(attr_list)   <span class="comment"># 获取所有的属性值</span></span><br><span class="line">    attr_nums = &#123;key:attr_list.count(key) <span class="keyword">for</span> key <span class="keyword">in</span> attrs &#125;  <span class="comment"># 得到属性对应的数目</span></span><br><span class="line">    attr_probs = [v/len(attr_list) <span class="keyword">for</span> k,v <span class="keyword">in</span> attr_nums.items()]   <span class="comment"># 得到属性可能性列表</span></span><br><span class="line">    entropy = sum([-p*log2(p) <span class="keyword">for</span> p <span class="keyword">in</span> attr_probs]);   <span class="comment"># 计算信息熵</span></span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"><span class="comment"># attr_list = ["youth","youth","middle_aged","senior","senior","senior","middle_aged","youth","youth","senior","youth","middle_aged","middle_aged","senior"];</span></span><br><span class="line"><span class="comment"># attr_list = ["no",'no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no'];</span></span><br><span class="line"><span class="comment"># get_shannon_entropy(attr_list)</span></span><br></pre></td></tr></table></figure>
<h2 id="1-1-使用信息增益选择属性"><a href="#1-1-使用信息增益选择属性" class="headerlink" title="1.1 使用信息增益选择属性"></a>1.1 使用信息增益选择属性</h2><p>按照步骤，计算对应数据集的属性的信息增益，选择最合适的属性作为分类属性。首先需要计算该属性划分数据集之后得到的对应的子数据集和子类型列表，之后在进行相应的信息熵的计算。</p>
<ul>
<li>划分数据集</li>
<li>计算信息增益，选择最佳属性</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_dataset</span><span class="params">(dataset, labels,feat_index)</span>:</span></span><br><span class="line">    <span class="string">""" 根据某个特征划分数据集</span></span><br><span class="line"><span class="string">    dataset：原始数据集，不包含标签</span></span><br><span class="line"><span class="string">    labels: 对应的标签</span></span><br><span class="line"><span class="string">    feat_index：特征在特征向量中的索引</span></span><br><span class="line"><span class="string">    return  ：返回以feat_index作为分类属性的时候，该属性对应的取值的子数据集和对应的类型。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dataset = copy.deepcopy(dataset)   <span class="comment"># 避免删除元素造成影响</span></span><br><span class="line">    <span class="comment"># 1、将每一列元素放在一个元组中，使用zip函数</span></span><br><span class="line">    dataset_zip =list( zip(*dataset))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#2、获取该特征列</span></span><br><span class="line">    feat_col = dataset_zip[feat_index];</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#3、根据该列特征的所有取值，构建一个字典，键为属性的取值，值为对应的数据集。</span></span><br><span class="line">    splited_dict = &#123;&#125;;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> set(feat_col):</span><br><span class="line">        splited_dict[item] = [[],[]];</span><br><span class="line">        </span><br><span class="line"><span class="comment">#     print(splited_dict)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(feat_col)):    <span class="comment"># 该特征列不能存在缺失，否则会有问题</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> splited_dict.keys():</span><br><span class="line">            <span class="keyword">if</span> feat_col[i] == key:</span><br><span class="line">                <span class="keyword">del</span> dataset[i][feat_index]      <span class="comment"># 删除该特征列的信息，表示该特征列已经用过了。</span></span><br><span class="line">                splited_dict[key][<span class="number">0</span>].append(dataset[i])</span><br><span class="line">                splited_dict[key][<span class="number">1</span>].append(labels[i])</span><br><span class="line">    <span class="keyword">return</span> splited_dict</span><br><span class="line">        </span><br><span class="line">split_dataset(dataset,labels,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据信息增益选择属性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_best_split_attr</span><span class="params">(dataset,labels)</span>:</span></span><br><span class="line">    <span class="string">"""dataset是数据集。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="comment">#     # 1、从带标签的数据集中取出标签列</span></span><br><span class="line"><span class="comment">#     labels = list(zip(*dataset))[-1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、计算labels中分类所需要的期望信息</span></span><br><span class="line">    entropy_all = get_shannon_entropy(labels)</span><br><span class="line"><span class="comment">#     print(entropy_all)</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 3、计算每个属性的期望信息需求</span></span><br><span class="line">    entropys = &#123;&#125;;   <span class="comment"># 存储对应的entropys</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(dataset[<span class="number">0</span>])):    <span class="comment"># 注意这里是使用1开始的，因为如果从0开始，那么第一个序号属性会使最好的分类属性，</span></span><br><span class="line">            <span class="comment"># 因为它将每一个数据都划分成一个类，这显然是没有意义的，这也是偏倚出现的原因，可以使用增益率作为属性选择计算方法来避免。</span></span><br><span class="line">        <span class="comment"># 划分数据集，返回的是一个字典</span></span><br><span class="line">        split_data = split_dataset(dataset,labels,i)</span><br><span class="line">        entropys[i] = [];</span><br><span class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> split_data.items():</span><br><span class="line">            <span class="comment"># split_data字典中，键是属性值，值=[子数据集，子类型列表]</span></span><br><span class="line">            k_rate = len(v[<span class="number">0</span>])/len(labels)    <span class="comment"># 该属性对应的数据集的个数占数据集总个数的比例</span></span><br><span class="line">            entropy = get_shannon_entropy(v[<span class="number">1</span>])   <span class="comment"># 计算该属性对应的数据标签的信息熵。</span></span><br><span class="line">            temp = k_rate*entropy;</span><br><span class="line">            entropys[i].append(temp)</span><br><span class="line">        entropys[i] = sum(entropys[i])    <span class="comment"># 计算属性i进行划分的时候的期望信息</span></span><br><span class="line"><span class="comment">#     print(entropys)</span></span><br><span class="line">    <span class="keyword">return</span> min(entropys,key=entropys.get)</span><br><span class="line">    </span><br><span class="line">choose_best_split_attr(dataset,labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比如以第三个属性作为特征值进行手动的计算entropy</span></span><br><span class="line">result = (<span class="number">7</span>/<span class="number">14</span>)*(-(<span class="number">4</span>/<span class="number">7</span>)*log2(<span class="number">4</span>/<span class="number">7</span>)-(<span class="number">3</span>/<span class="number">7</span>)*log2(<span class="number">3</span>/<span class="number">7</span>))+(<span class="number">7</span>/<span class="number">14</span>)*(-(<span class="number">6</span>/<span class="number">7</span>)*log2(<span class="number">6</span>/<span class="number">7</span>)-(<span class="number">1</span>/<span class="number">7</span>)*log2(<span class="number">1</span>/<span class="number">7</span>))</span><br><span class="line">result     <span class="comment"># 与上面计算的结果一致，可以看出算法正确</span></span><br></pre></td></tr></table></figure>
<h1 id="2-树分裂"><a href="#2-树分裂" class="headerlink" title="2 树分裂"></a>2 树分裂</h1><p>有了选取最佳分裂属性的算法，接着就开始使用选择的属性来将树进一步的分裂。所谓树的分裂只不过是根据选择的属性将数据集划分，然后在总划分出来的数据集中再次调用选取属性的方法选择子数据集的最佳属性，最好的实现方式就是递归了。<br>python中使用什么数据结构表示决策树？可以使用字典很方便的表示决策树的嵌套，一个树的根节点便是属性，属性对应的值又是一个新的字典，其中key为属性的可能值，value为子树。</p>
<h2 id="2-1-得到占据大多数的类型"><a href="#2-1-得到占据大多数的类型" class="headerlink" title="2.1 得到占据大多数的类型"></a>2.1 得到占据大多数的类型</h2><p>当所有属性都使用完的时候还没有将数据完全分开，此时就返回所占比重较大的那个分类。比如使用完最后一个属性，得到的labels列表为[‘yes’,’yes’,’no’]那么返回的就是yes类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_majority</span><span class="params">(labels)</span>:</span></span><br><span class="line">    label_nums = defaultdict(<span class="keyword">lambda</span>:<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        label_nums[label] += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> max(label_nums, key=label_nums.get)</span><br><span class="line"></span><br><span class="line"><span class="comment"># labels = ['yes','yes','no'];</span></span><br><span class="line"><span class="comment"># print(get_majority(labels));</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-创建树"><a href="#2-2-创建树" class="headerlink" title="2.2 创建树"></a>2.2 创建树</h2><p>树分裂终止的两个条件是：</p>
<ul>
<li>遍历完所有的属性。在进行树分裂的时候，我们数据集中数据向量的属性是不断缩短的，当缩短到1的时候（数据向量中包括labels，所以这里为1.否则为0），说明数据集中的属性已被全部使用完毕，便不能再分裂下去了，此时我们选取最终子数据集中的众数作为最终的分类结果放在叶子节点上。</li>
<li>新划分的数据集中只有一个属性。所该节点下面的子数据集中labels是一致的，那么就不用在进行分类了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(dataset, labels, feat_names)</span>:</span></span><br><span class="line">    <span class="string">"""dataset是含有标签的数据集，labels是对应的标签，feat_names是数据集中数据相应的特征属性</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> Tree;</span><br><span class="line">    <span class="keyword">if</span> (len(set(labels)) == <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 如果数据集中只有一个类型，就可以停止分裂</span></span><br><span class="line">        <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(feat_names) == <span class="number">1</span>:  <span class="comment"># 因为没有考虑第一个属性RID</span></span><br><span class="line">        <span class="comment"># 如果属性已经用完了，那么返回比例最多的类型。</span></span><br><span class="line">        <span class="keyword">return</span> get_majority(labels)</span><br><span class="line"></span><br><span class="line">    tree = &#123;&#125;;</span><br><span class="line">    best_feature_index = choose_best_split_attr(dataset, labels);  <span class="comment"># 获取最好的分类属性的index</span></span><br><span class="line">    feature = feat_names[best_feature_index];  <span class="comment"># 获取属性index对应的属性名</span></span><br><span class="line">    tree[feature] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    sub_feat_names = feat_names[:];</span><br><span class="line">    <span class="comment">#     print(sub_feat_names[best_feature_index])</span></span><br><span class="line">    sub_feat_names.pop(best_feature_index);</span><br><span class="line"></span><br><span class="line">    splited_dict = split_dataset(dataset, labels, best_feature_index);  <span class="comment"># 根据该属性分类，得到分类之后的数据集</span></span><br><span class="line">    <span class="keyword">for</span> feat_col, (sub_dataset, sub_labels) <span class="keyword">in</span> splited_dict.items():</span><br><span class="line">        tree[feature][feat_col] = create_tree(sub_dataset, sub_labels, sub_feat_names)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tree;</span><br><span class="line"></span><br><span class="line">create_tree(dataset,labels,feat_names)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;age&apos;: {&apos;middle_aged&apos;: &apos;yes&apos;,
  &apos;senior&apos;: {&apos;credit_rating&apos;: {&apos;excellent&apos;: &apos;no&apos;, &apos;fair&apos;: &apos;yes&apos;}},
  &apos;youth&apos;: {&apos;student&apos;: {&apos;no&apos;: &apos;no&apos;, &apos;yes&apos;: &apos;yes&apos;}}}}
</code></pre><h1 id="3-合并所有代码"><a href="#3-合并所有代码" class="headerlink" title="3 合并所有代码"></a>3 合并所有代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *;</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">filename = <span class="string">r'C:\Users\14259\Desktop\24周\electronics.csv'</span>;</span><br><span class="line">feat_names = [<span class="string">'RID'</span>, <span class="string">'age'</span>, <span class="string">'income'</span>, <span class="string">'student'</span>, <span class="string">'credit_rating'</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    csv_reader = csv.reader(open(filename, <span class="string">'r'</span>));</span><br><span class="line">    dataset = list(csv_reader)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集中属性和labels切分开</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_lables</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataattrs = list(zip(*dataset))</span><br><span class="line">    labels = dataattrs[<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="keyword">del</span> item[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算信息熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_shannon_entropy</span><span class="params">(attr_list)</span>:</span></span><br><span class="line">    attrs = set(attr_list)  <span class="comment"># 获取所有的属性值</span></span><br><span class="line">    attr_nums = &#123;key: attr_list.count(key) <span class="keyword">for</span> key <span class="keyword">in</span> attrs&#125;  <span class="comment"># 得到属性对应的数目</span></span><br><span class="line">    attr_probs = [v / len(attr_list) <span class="keyword">for</span> k, v <span class="keyword">in</span> attr_nums.items()]  <span class="comment"># 得到属性可能性列表</span></span><br><span class="line">    entropy = sum([-p * log2(p) <span class="keyword">for</span> p <span class="keyword">in</span> attr_probs]);  <span class="comment"># 计算信息熵</span></span><br><span class="line">    <span class="keyword">return</span> entropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_dataset</span><span class="params">(dataset, labels, feat_index)</span>:</span></span><br><span class="line">    <span class="string">""" 根据某个特征划分数据集</span></span><br><span class="line"><span class="string">    dataset：原始数据集，不包含标签</span></span><br><span class="line"><span class="string">    labels: 对应的标签</span></span><br><span class="line"><span class="string">    feat_index：特征在特征向量中的索引</span></span><br><span class="line"><span class="string">    return  ：返回以feat_index作为分类属性的时候，该属性对应的取值的子数据集和对应的类型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dataset = copy.deepcopy(dataset)  <span class="comment"># 避免删除元素造成影响</span></span><br><span class="line">    <span class="comment"># 1、将每一列元素放在一个元组中，使用zip函数</span></span><br><span class="line">    dataset_zip = list(zip(*dataset))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、获取该特征列</span></span><br><span class="line">    feat_col = dataset_zip[feat_index];</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3、根据该列特征的所有取值，构建一个字典，键为属性的取值，值为对应的数据集。</span></span><br><span class="line">    splited_dict = &#123;&#125;;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> set(feat_col):</span><br><span class="line">        splited_dict[item] = [[], []];</span><br><span class="line"></span><br><span class="line">    <span class="comment">#     print(splited_dict)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(feat_col)):  <span class="comment"># 该特征列不能存在缺失，否则会有问题</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> splited_dict.keys():</span><br><span class="line">            <span class="keyword">if</span> feat_col[i] == key:</span><br><span class="line">                <span class="keyword">del</span> dataset[i][feat_index]  <span class="comment"># 删除该特征列的信息，表示该特征列已经用过了。</span></span><br><span class="line">                splited_dict[key][<span class="number">0</span>].append(dataset[i])</span><br><span class="line">                splited_dict[key][<span class="number">1</span>].append(labels[i])</span><br><span class="line">    <span class="keyword">return</span> splited_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据信息增益选择属性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_best_split_attr</span><span class="params">(dataset, labels)</span>:</span></span><br><span class="line">    <span class="string">"""dataset是数据集。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#     # 1、从带标签的数据集中取出标签列</span></span><br><span class="line">    <span class="comment">#     labels = list(zip(*dataset))[-1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、计算labels中分类所需要的期望信息</span></span><br><span class="line">    entropy_all = get_shannon_entropy(labels)</span><br><span class="line">    <span class="comment">#     print(entropy_all)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3、计算每个属性的期望信息需求</span></span><br><span class="line">    entropys = &#123;&#125;;  <span class="comment"># 存储对应的entropys</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(dataset[<span class="number">0</span>])):  <span class="comment"># 注意这里是使用1开始的，因为如果从0开始，那么第一个序号属性会使最好的分类属性，</span></span><br><span class="line">        <span class="comment"># 因为它将每一个数据都划分成一个类，这显然是没有意义的，这也是偏倚出现的原因，可以使用增益率作为属性选择计算方法来避免。</span></span><br><span class="line">        <span class="comment"># 划分数据集，返回的是一个字典</span></span><br><span class="line">        split_data = split_dataset(dataset, labels, i)</span><br><span class="line">        entropys[i] = [];</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> split_data.items():</span><br><span class="line">            <span class="comment"># split_data字典中，键是属性值，值=[子数据集，子类型列表]</span></span><br><span class="line">            k_rate = len(v[<span class="number">0</span>]) / len(labels)  <span class="comment"># 该属性对应的数据集的个数占数据集总个数的比例</span></span><br><span class="line">            entropy = get_shannon_entropy(v[<span class="number">1</span>])  <span class="comment"># 计算该属性对应的数据标签的信息熵。</span></span><br><span class="line">            temp = k_rate * entropy;</span><br><span class="line">            entropys[i].append(temp)</span><br><span class="line">        entropys[i] = sum(entropys[i])  <span class="comment"># 计算属性i进行划分的时候的期望信息</span></span><br><span class="line">    <span class="comment">#     print(entropys)</span></span><br><span class="line">    <span class="keyword">return</span> min(entropys, key=entropys.get)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_majority</span><span class="params">(labels)</span>:</span></span><br><span class="line">    label_nums = defaultdict(<span class="keyword">lambda</span>: <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        label_nums[label] += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> max(label_nums, key=label_nums.get)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(dataset, labels, feat_names)</span>:</span></span><br><span class="line">    <span class="string">"""dataset是含有标签的数据集，labels是对应的标签，feat_names是数据集中数据相应的特征属性</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> Tree;</span><br><span class="line">    <span class="keyword">if</span> (len(set(labels)) == <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 如果数据集中只有一个类型，就可以停止分裂</span></span><br><span class="line">        <span class="keyword">return</span> labels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(feat_names) == <span class="number">1</span>:  <span class="comment"># 因为没有考虑第一个属性RID</span></span><br><span class="line">        <span class="comment"># 如果属性已经用完了，那么返回比例最多的类型。</span></span><br><span class="line">        <span class="keyword">return</span> get_majority(labels)</span><br><span class="line"></span><br><span class="line">    tree = &#123;&#125;;</span><br><span class="line">    best_feature_index = choose_best_split_attr(dataset, labels);  <span class="comment"># 获取最好的分类属性的index</span></span><br><span class="line">    feature = feat_names[best_feature_index];  <span class="comment"># 获取属性index对应的属性名</span></span><br><span class="line">    tree[feature] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">    sub_feat_names = feat_names[:];</span><br><span class="line">    <span class="comment">#     print(sub_feat_names[best_feature_index])</span></span><br><span class="line">    sub_feat_names.pop(best_feature_index);</span><br><span class="line"></span><br><span class="line">    splited_dict = split_dataset(dataset, labels, best_feature_index);  <span class="comment"># 根据该属性分类，得到分类之后的数据集</span></span><br><span class="line">    <span class="keyword">for</span> feat_col, (sub_dataset, sub_labels) <span class="keyword">in</span> splited_dict.items():</span><br><span class="line">        tree[feature][feat_col] = create_tree(sub_dataset, sub_labels, sub_feat_names)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tree;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = load_data(filename);</span><br><span class="line">    print(<span class="string">"data size is %d lines"</span> % len(dataset))</span><br><span class="line">    dataset, labels = split_lables(dataset)</span><br><span class="line">    tree = create_tree(dataset, labels, feat_names)</span><br><span class="line">    print(<span class="string">"生成的决策树如下:"</span>)</span><br><span class="line">    print(tree)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<pre><code>data size is 14 lines
生成的决策树如下:
{&apos;age&apos;: {&apos;senior&apos;: {&apos;credit_rating&apos;: {&apos;excellent&apos;: &apos;no&apos;, &apos;fair&apos;: &apos;yes&apos;}}, &apos;youth&apos;: {&apos;student&apos;: {&apos;yes&apos;: &apos;yes&apos;, &apos;no&apos;: &apos;no&apos;}}, &apos;middle_aged&apos;: &apos;yes&apos;}}
</code></pre><h1 id="4-可视化决策树"><a href="#4-可视化决策树" class="headerlink" title="4 可视化决策树"></a>4 可视化决策树</h1><p>通过嵌套字典表示决策树对人来说不好理解，可以借助可视化工具将该结构可视化。使用Graphviz来可视化树结构。因此，首先需要将字典表示的树生成 Graphviz Dot文件内容的函数，思想就是递归获取整棵树的所有节点和连接节点的边然后将这些节点和边生成Dot格式的字符串写入到文件中，然后进行图形的绘制。<br><strong>这一部分内容暂时没有做，需要安装Graphviz软件以及学习使用。</strong></p>
<h1 id="5-使用生成的决策树进行分类"><a href="#5-使用生成的决策树进行分类" class="headerlink" title="5 使用生成的决策树进行分类"></a>5 使用生成的决策树进行分类</h1><p>对未知数据进行预测，主要是根据树中的结点递归找到叶子结点即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(data_vect,feat_names,tree)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>(type(tree) <span class="keyword">is</span> <span class="keyword">not</span> dict):</span><br><span class="line">        <span class="comment"># 说明找到了叶子结点</span></span><br><span class="line">        <span class="keyword">return</span> tree;  </span><br><span class="line">    </span><br><span class="line">    feature = list(tree.keys())[<span class="number">0</span>]   <span class="comment"># 获取树的最顶层的属性</span></span><br><span class="line">    value = data_vect[feat_names.index(feature)]  <span class="comment">#获取测试数据该属性对应的值</span></span><br><span class="line">    sub_tree = tree[feature][value]</span><br><span class="line">    <span class="keyword">return</span> classify(data_vect,feat_names,sub_tree)</span><br><span class="line"><span class="comment"># data_vect = [14,'senior','medium','no','fair'];</span></span><br><span class="line"><span class="comment"># label = classify(data_vect,feat_names,tree);</span></span><br><span class="line"><span class="comment"># print(label)</span></span><br></pre></td></tr></table></figure>
<pre><code>yes
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_accuracy</span><span class="params">(dataset,labels,feat_names,tree)</span>:</span></span><br><span class="line">    predict_labels = [];</span><br><span class="line">    <span class="keyword">for</span> data_vect <span class="keyword">in</span> dataset:</span><br><span class="line">        label = classify(data_vect,feat_names,tree)</span><br><span class="line">        predict_labels.append(label)</span><br><span class="line">    </span><br><span class="line">    count = <span class="number">0</span>;</span><br><span class="line">    labels =list(zip(predict_labels,labels))</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        <span class="keyword">if</span> label[<span class="number">0</span>] == label[<span class="number">1</span>]:</span><br><span class="line">            count += <span class="number">1</span>;</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> count/len(dataset)</span><br><span class="line"></span><br><span class="line">dataset =[[<span class="string">'1'</span>, <span class="string">'youth'</span>, <span class="string">'high'</span>, <span class="string">'no'</span>, <span class="string">'fair'</span>], [<span class="string">'2'</span>, <span class="string">'youth'</span>, <span class="string">'high'</span>, <span class="string">'no'</span>, <span class="string">'excellent'</span>], [<span class="string">'3'</span>, <span class="string">'middle_aged'</span>, <span class="string">'high'</span>, <span class="string">'no'</span>, <span class="string">'fair'</span>], [<span class="string">'4'</span>, <span class="string">'senior'</span>, <span class="string">'medium'</span>, <span class="string">'no'</span>, <span class="string">'fair'</span>], [<span class="string">'5'</span>, <span class="string">'senior'</span>, <span class="string">'low'</span>, <span class="string">'yes'</span>, <span class="string">'fair'</span>], [<span class="string">'6'</span>, <span class="string">'senior'</span>, <span class="string">'low'</span>, <span class="string">'yes'</span>, <span class="string">'excellent'</span>]]</span><br><span class="line">labels = [<span class="string">'no'</span>, <span class="string">'no'</span>, <span class="string">'no'</span>, <span class="string">'no'</span>, <span class="string">'yes'</span>, <span class="string">'no'</span>, <span class="string">'yes'</span>]</span><br><span class="line">accuracy = predict_accuracy(dataset,labels,feat_names,tree)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"准确率是: %.4f"</span> % accuracy)</span><br></pre></td></tr></table></figure>
<pre><code>准确率是: 0.6667
</code></pre><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h1><p>本文一步步实现了一个基本的决策树分类算法，里面还有较多不完善的地方。</p>
<ul>
<li>比如在属性选择方面，使用的是信息增益，可能存在偏倚问题。之后可以通过增益率作为属性选择度量。</li>
<li>如果测试数据中出现了训练数据中没有出现的某个属性的类别，那么会出现错误。</li>
<li>本文没有涉及到树剪枝等问题，可能是数据集比较简单，没有遇到该类问题。<br>写本文主要是为了熟悉决策树的一个分类的原理，从而可以更好的理解，之后在进行使用的时候可以使用目前比较完善的，比如scikit-learn</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/09/MachineLearningAndDataMining/决策树/" data-id="cjtinfpse005exw772j10dh64" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/决策树/">决策树</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/分类/">分类</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/08/10/java/java基础知识系列（一）/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          java基础系列知识——Snippets
        
      </div>
    </a>
  
  
    <a href="/2018/08/07/MachineLearningAndDataMining/朴素贝叶斯/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">朴素贝叶斯分类代码实现</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C语言/">C语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kaggle/">kaggle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python3/">python3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/weibo/">weibo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/word2vec/">word2vec</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据挖掘/">数据挖掘</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BP算法/">BP算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C语言/">C语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/">DL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM/">EM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IO/">IO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/">KNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SPSS/">SPSS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/beautifulsoup/">beautifulsoup</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/collections/">collections</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/divide-and-conquer/">divide-and-conquer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/file/">file</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jieba/">jieba</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k-means/">k-means</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kaggle/">kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/matplotlib/">matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/numpy/">numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pandas/">pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-爬虫/">python 爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python3/">python3</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/realize/">realize</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/requests/">requests</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sklearn/">sklearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/snippets/">snippets</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/weibo/">weibo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/wiebo/">wiebo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/全概率公式/">全概率公式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/决策树/">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分类/">分类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/动态规划/">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/字符串/">字符串</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/小demo/">小demo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具/">工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/微博/">微博</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/支持向量机/">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据分析/">数据分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/时间复杂度/">时间复杂度</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/最大似然估计/">最大似然估计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贝叶斯/">贝叶斯</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 17.5px;">Algorithm</a> <a href="/tags/BP算法/" style="font-size: 10px;">BP算法</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/C语言/" style="font-size: 10px;">C语言</a> <a href="/tags/DL/" style="font-size: 10px;">DL</a> <a href="/tags/EM/" style="font-size: 10px;">EM</a> <a href="/tags/IO/" style="font-size: 10px;">IO</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/MachineLearning/" style="font-size: 12.5px;">MachineLearning</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/SPSS/" style="font-size: 10px;">SPSS</a> <a href="/tags/beautifulsoup/" style="font-size: 10px;">beautifulsoup</a> <a href="/tags/collections/" style="font-size: 10px;">collections</a> <a href="/tags/divide-and-conquer/" style="font-size: 10px;">divide-and-conquer</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/jieba/" style="font-size: 10px;">jieba</a> <a href="/tags/k-means/" style="font-size: 10px;">k-means</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/matplotlib/" style="font-size: 12.5px;">matplotlib</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/numpy/" style="font-size: 10px;">numpy</a> <a href="/tags/pandas/" style="font-size: 10px;">pandas</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/python-爬虫/" style="font-size: 10px;">python 爬虫</a> <a href="/tags/python3/" style="font-size: 10px;">python3</a> <a href="/tags/realize/" style="font-size: 10px;">realize</a> <a href="/tags/requests/" style="font-size: 10px;">requests</a> <a href="/tags/sklearn/" style="font-size: 10px;">sklearn</a> <a href="/tags/snippets/" style="font-size: 10px;">snippets</a> <a href="/tags/weibo/" style="font-size: 10px;">weibo</a> <a href="/tags/wiebo/" style="font-size: 10px;">wiebo</a> <a href="/tags/word2vec/" style="font-size: 10px;">word2vec</a> <a href="/tags/全概率公式/" style="font-size: 10px;">全概率公式</a> <a href="/tags/决策树/" style="font-size: 10px;">决策树</a> <a href="/tags/分类/" style="font-size: 12.5px;">分类</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/字符串/" style="font-size: 10px;">字符串</a> <a href="/tags/小demo/" style="font-size: 10px;">小demo</a> <a href="/tags/工具/" style="font-size: 10px;">工具</a> <a href="/tags/微博/" style="font-size: 10px;">微博</a> <a href="/tags/支持向量机/" style="font-size: 10px;">支持向量机</a> <a href="/tags/数据分析/" style="font-size: 10px;">数据分析</a> <a href="/tags/时间复杂度/" style="font-size: 10px;">时间复杂度</a> <a href="/tags/最大似然估计/" style="font-size: 10px;">最大似然估计</a> <a href="/tags/算法/" style="font-size: 12.5px;">算法</a> <a href="/tags/贝叶斯/" style="font-size: 10px;">贝叶斯</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/21/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2018/11/02/Algorithms/DivideAndConquer/">Divide and Conquer</a>
          </li>
        
          <li>
            <a href="/2018/11/02/Algorithms/Dynamic-Planning/">Dynamic Planning</a>
          </li>
        
          <li>
            <a href="/2018/10/31/python3/word2vec/">word2vec</a>
          </li>
        
          <li>
            <a href="/2018/10/21/Algorithms/算法/">算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>